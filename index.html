<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<div id="layout-content">
<h1>Junchi Yang </h1>
<table class="imgtable"><tr><td>
<img src="junchi.jpg" alt="alt text" width="130px" height="183.5" />&nbsp;</td>
<td align="left"><p>I am an Assistant Professor in the School of Data Science at the Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen). My research interests lie in optimization and its applications in machine learning and power systems. Before joining CUHK-Shenzhen, I am a postdoctoral researcher at Argonne National Laboratory. I completed my Ph.D. in Computer Science at ETH Zurich, under the supervision of <a href="https://odi.inf.ethz.ch/niaohe" target=&ldquo;blank&rdquo;>Niao He</a>. I received a master's degree in Industrial Engineering from the University of Illinois Urbana-Champaign and a bachelor’s degree in Applied Mathematics and Economics from UCLA.
</p>
<p>Email: yangjunchi@cuhk.edu.cn
</p>
<p><a href="https://www.linkedin.com/in/junchi-yang-455206b0/" target=&ldquo;blank&rdquo;>Linkedin</a>. <a href="https://scholar.google.com/citations?user=Av7wBOQAAAAJ&amp;hl=en/" target=&ldquo;blank&rdquo;>Google Scholar</a>.
</p>
</td></tr></table>
<h2>Prospective Students</h2>
<p>I am seeking highly motivated Ph.D. students, Master’s students, and interns with a strong background in mathematics and/or solid programming skills. If you are interested in mathematical optimization and its applications in machine learning and power systems, please feel free to email me your CV and any other supporting documents. Students from diverse fields, including mathematics, electrical engineering, computer science, and related disciplines, are warmly encouraged to apply. 
</p>
<h2>News</h2>
<ul>
<li><p>[2024.6] I am attending the 2024 IEEE Power &amp; Energy Society General Meeting in July.
</p>
</li>
</ul>
<ul>
<li><p>[2024.2] I am attending the 2024 Conference on Innovative Smart Grid Technologies (ISGT) in February and the INFORMS Optimization Society Conference in March.
</p>
</li>
</ul>
<ul>
<li><p>[2024.2] My Ph.D. Thesis &ldquo;<a href="https://www.research-collection.ethz.ch/handle/20.500.11850/658589" target=&rdquo;blank&ldquo;>Towards Near-Optimal and Adaptive Algorithms in Minimax Optimization</a>&rdquo; is now available online. Check it out.
</p>
</li>
</ul>
<ul>
<li><p>[2024.1] Our paper &ldquo;Parameter-Agnostic Optimization under Relaxed Smoothness&rdquo; is accepted to AISTATS 2024. 
</p>
</li>
</ul>
<ul>
<li><p>[2024.1] I started as a postdoctoral researcher at Argonne National Laboratory.
</p>
</li>
</ul>
<ul>
<li><p>[2023.09] Two co-first-authored papers are accepted to NeurIPS 2023. One of them is selected to be a spotlight paper. 
</p>
</li>
</ul>
<ul>
<li><p>[2023.09] I've successfully defended my Ph.D. thesis. Grateful for this academic journey.
</p>
</li>
</ul>
<h2>Publications </h2>
<p>(* indicates equal contribution) 
</p>
<p><br />
</p>
<ul>
<li><p>Accelerating Distributed Optimization: A Primal-Dual Perspective on Local Steps<br />
<b>Junchi Yang</b>, Murat Yildirim, Qiu Feng.<br />
Manuscript 2024 <a href="https://arxiv.org/pdf/2407.02689" target=&ldquo;blank&rdquo;>[arXiv]</a> 
</p>
</li>
</ul>
<ul>
<li><p>Parameter-Agnostic Optimization under Relaxed Smoothness<br />
Florian Hübler, <b>Junchi Yang</b>, Xiang Li, Niao He.<br />
AISTATS 2024 (preliminary version in NeurIPS OPT Workshop 2023) <a href="https://arxiv.org/pdf/2311.03252.pdf" target=&ldquo;blank&rdquo;>[arXiv]</a> 
</p>
</li>
</ul>
<ul>
<li><p>Optimal Guarantees for Algorithmic Reproducibility and Gradient Complexity in Convex Optimization<br />
Liang Zhang*, <b>Junchi Yang</b>*, Amin Karbasi, Niao He.<br />
NeurIPS 2023 (<b>spotlight</b>). <a href="https://arxiv.org/pdf/2310.17759.pdf" target=&ldquo;blank&rdquo;>[arXiv]</a>  <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/38c5feed4b72c96f6cf925ccc9832ecf-Abstract-Conference.html" target=&ldquo;blank&rdquo;>[NeurIPS]</a>
</p>
</li>
</ul>
<ul>
<li><p>Two Sides of One Coin: the Limits of Untuned SGD and the Power of Adaptive Methods<br />
<b>Junchi Yang</b>*, Xiang Li*, Ilyas Fatkhullin, and Niao He.<br />
NeurIPS 2023. <a href="https://arxiv.org/pdf/2305.12475.pdf" target=&ldquo;blank&rdquo;>[arXiv]</a>  <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/eb1a323fa10d4102ff13422476a744ff-Abstract-Conference.html" target=&ldquo;blank&rdquo;>[NeurIPS]</a>
</p>
</li>
</ul>
<ul>
<li><p>TiAda: A Time-Scale Adaptive Algorithm For Nonconvex Minimax Optimization<br />
Xiang Li, <b>Junchi Yang</b>, and Niao He.<br />
ICLR 2023 (preliminary version in NeurIPS OPT Workshop 2022) <a href="https://arxiv.org/pdf/2210.17478.pdf" target=&ldquo;blank&rdquo;>[arXiv]</a>   <a href="https://openreview.net/pdf?id=zClyiZ5V6sL" target=&ldquo;blank&rdquo;>[ICLR]</a>
</p>
</li>
</ul>
<ul>
<li><p>Nest Your Adaptive Algorithm for Parameter-Agnostic Nonconvex Minimax Optimization<br />
<b>Junchi Yang</b>*,  Xiang Li*, and Niao He.<br />
NeurIPS 2022 <a href="https://arxiv.org/abs/2206.00743" target=&ldquo;blank&rdquo;>[arXiv]</a>  <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/488b8db9ec118c3d750c34d1812a5a3a-Abstract-Conference.html" target=&ldquo;blank&rdquo;>[NeurIPS]</a>
</p>
</li>
</ul>
<ul>
<li><p>Faster Single-Loop Algorithms for Minimax Optimization without Strong Concavity<br />
<b>Junchi Yang</b>, Antonio Orvieto, Aurelien Lucchi, and Niao He.<br />
AISTATS 2022 <a href="https://arxiv.org/abs/2112.05604" target=&ldquo;blank&rdquo;>[arXiv]</a> <a href="https://proceedings.mlr.press/v151/yang22b.html" target=&ldquo;blank&rdquo;>[AISTATS]</a>
</p>
</li>
</ul>
<ul>
<li><p>The Complexity of Nonconvex-Strongly-Concave Minimax Optimization<br />
Siqi Zhang*,  <b>Junchi Yang</b>*, Cristobal Guzman, Negar Kiyavash and Niao He.<br />
UAI 2021 <a href="https://arxiv.org/abs/2103.15888" target=&ldquo;blank&rdquo;>[arXiv]</a> <a href="https://www.auai.org/uai2021/pdf/uai2021.205.pdf" target=&ldquo;blank&rdquo;>[UAI]</a>
</p>
</li>
</ul>
<ul>
<li><p>A Catalyst Framework for Minimax Optimization<br />
<b>Junchi Yang</b>, Siqi Zhang, Negar Kiyavash, and Niao He.<br />
NeurIPS 2020 <a href="https://proceedings.neurips.cc/paper/2020/hash/3db54f5573cd617a0112d35dd1e6b1ef-Abstract.html" target=&ldquo;blank&rdquo;>[NeurIPS]</a>
</p>
</li>
</ul>
<ul>
<li><p>Global Convergence and Variance Reduction for a Class of Nonconvex-Nonconcave Minimax Problems<br />
<b>Junchi Yang</b>, Negar Kiyavash, and Niao He.<br />
NeurIPS 2020 <a href="https://arxiv.org/abs/2002.09621" target=&ldquo;blank&rdquo;>[arXiv]</a> <a href="https://proceedings.neurips.cc/paper/2020/hash/0cc6928e741d75e7a92396317522069e-Abstract.html" target=&ldquo;blank&rdquo;>[NeurIPS]</a>
</p>
</li>
</ul>
<h2>Thesis</h2>
<ul>
<li><p>Towards Near-Optimal and Adaptive Algorithms in Minimax Optimization<br />
Doctoral Thesis. ETH Zurich. 2023 <a href="https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/658589/PhD_Thesis_twosided.pdf?sequence=1" target=&ldquo;blank&rdquo;>[ETH Research Collection]</a><br />
(Chapter 2 of the thesis presents our complete Catalyst framework for unbalanced minimax problems. The full results were initially presented at the <a href="https://publish.illinois.edu/csl-student-conference-2021/program/technical-sessions/control-systems-and-reinforcement-learning/" target=&ldquo;blank&rdquo;>CSL Student Conference 2021</a>, and portions appeared in our <a href="https://proceedings.neurips.cc/paper/2020/hash/3db54f5573cd617a0112d35dd1e6b1ef-Abstract.html" target=&ldquo;blank&rdquo;>NeurIPS 2020</a> and <a href="https://www.auai.org/uai2021/pdf/uai2021.205.pdf" target=&ldquo;blank&rdquo;>UAI 2021</a> papers.)
</p>
</li>
</ul>
<h2>Talks</h2>
<ul>
<li><p>2024 INFORMS Optimization Society Conference (IOS). Title: &ldquo;Optimal Guarantees for Algorithmic Reproducibility and Gradient Complexity in Convex Optimization&rdquo;.
</p>
</li>
</ul>
<ul>
<li><p>20th EUROpt Workshop 2023. Title: &ldquo;From SGD to Adaptive Methods: Benefits of Adaptive Gradient Techniques&rdquo;.
</p>
</li>
</ul>
<ul>
<li><p>SIAM Conference on Optimization 2023 (OP23). Title: &ldquo;Adaptive Algorithms for Nonconvex Minimax Optimization&rdquo;.
</p>
</li>
</ul>
<ul>
<li><p>CSL Student Conference 2021. Title: &ldquo;A Catalyst Framework for Minimax Optimization&rdquo;.
</p>
</li>
</ul>
<ul>
<li><p>INFORMS 2020 Annual Meeting. Title: &ldquo;Simple and Efficient Algorithms for Classes of Nonconvex Minimax Optimization&rdquo;.
</p>
</li>
</ul>
<h2>Teaching</h2>
<p>
</p>
<ul>
<li><p>Optimization for Data Science. ETH Zurich. 2022 Spring, 2023 Winter.
</p>
</li>
</ul>
<ul>
<li><p>Foundations of Reinforcement Learning. ETH Zurich. 2021 Fall.
</p>
</li>
</ul>
<ul>
<li><p>Analysis of Data. UIUC. 2019 Fall. 
</p>
</li>
</ul>
<h2>Service</h2>
<p>Reviewer for NeurIPS, ICML, ICLR, AISTATS and TMLR.
</p>
<h2>Miscellaneous</h2>
<ul>
<li><p>I received ISE Student Fellowship at UIUC 2017-2018. 
</p>
</li>
</ul>
<ul>
<li><p>Top reviewer for NeurIPS 2022.
</p>
</li>
</ul>
<ul>
<li><p>I play <a href="https://raw.githubusercontent.com/Junchi-Michael-Yang/Junchi-Michael-Yang.github.io/main/volleyball2.jpg" target=&ldquo;blank&rdquo;>volleyball</a> and used to play badminton and table tennis. I am learning <a href="https://raw.githubusercontent.com/Junchi-Michael-Yang/Junchi-Michael-Yang.github.io/main/snowboarding.JPG" target=&ldquo;blank&rdquo;>snowboarding</a> and tennis.
</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2024-12-11 17:35:50 CST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a> and <a href="https://openai.com/blog/chatgpt">ChatGPT</a>.
</div>
</div>
</div>
</body>
</html>
